{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tabulate import tabulate\n",
    "import io\n",
    "import copy\n",
    "import re\n",
    "# Azure imports\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient \n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from azure.search.documents import SearchClient, IndexDocumentsBatch\n",
    "from azure.search.documents.indexes.models import(\n",
    "    SimpleField,\n",
    "    ComplexField,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .env Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_env():\n",
    "    from dotenv import load_dotenv, dotenv_values\n",
    "    import os\n",
    "\n",
    "    # Clear the current environment variables\n",
    "    for key in dotenv_values().keys():\n",
    "        os.environ.pop(key, None)\n",
    "\n",
    "    # Reload the .env file\n",
    "    load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code loads environment variables using the `dotenv` library and sets the necessary environment variables for Azure services.\n",
    "The environment variables are loaded from the `.env` file in the same directory as this notebook.\n",
    "\"\"\"\n",
    "\n",
    "#load_dotenv()\n",
    "reload_env()\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"OPENAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"OPENAI_KEY\")\n",
    "doc_intelligence_endpoint = os.getenv(\"DOC_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"DOC_INTELLIGENCE_KEY\")\n",
    "blob_connection_string = os.getenv(\"BLOB_STORAGE_CONNECTION_STRING\")\n",
    "storage_account_key = os.getenv(\"STORAGE_ACCOUNT_KEY\")\n",
    "search_endpoint: str = os.getenv(\"AI_SEARCH_ENDPOINT\")\n",
    "search_admin_key: str = os.getenv(\"AI_SEARCH_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index name\n",
    "index_name: str = \"docu-layout-01\" #Change to index name that you want to store to\n",
    "\n",
    "# Connect to document intelligence\n",
    "document_intelligence_client = DocumentIntelligenceClient(endpoint= doc_intelligence_endpoint, credential=AzureKeyCredential(doc_intelligence_key))\n",
    "\n",
    "# Connect to blob storage \n",
    "blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
    "\n",
    "blob_client = blob_service_client.get_container_client(\"docu-layout-01\")\n",
    "if not blob_client.exists():\n",
    "    blob_client.create_container()\n",
    "    print(\"Created\")\n",
    "list_blobs = blob_client\n",
    "\n",
    "# Connect to search service \n",
    "search_client = SearchClient(endpoint = search_endpoint, index_name = index_name, credential = AzureKeyCredential(search_admin_key))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: Coen424 Fall 2024-Assignment One.pdf as <azure.storage.blob._blob_client.BlobClient object at 0x00000200FECC3E30>\n"
     ]
    }
   ],
   "source": [
    "path = \"D:\\\\OneDrive\\\\Semester 7 COEN\\\\COEN 424\\\\Project\\\\Code\\\\COEN-424-DocuLayout\\\\Coen424 Fall 2024-Assignment One.pdf\"\n",
    "# Get files \n",
    "files = [\"D:\\\\OneDrive\\\\Semester 7 COEN\\\\COEN 424\\\\Project\\\\Code\\\\COEN-424-DocuLayout\\\\Coen424 Fall 2024-Assignment One.pdf\"]\n",
    "\n",
    "# Send files\n",
    "for index, file_to_send in enumerate(files):\n",
    "    with open(file_to_send, mode = \"rb\") as data:\n",
    "        sent = blob_client.upload_blob(name=os.path.basename(file_to_send), data=data, overwrite=True) #should also add metadata\n",
    "    print(f\"Sent: {os.path.basename(file_to_send)} as {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect openAI and search service \n",
    "aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-3-small\",\n",
    "    openai_api_version=\"2024-05-01-preview\",  # e.g., \"2023-12-01-preview\"\n",
    ")\n",
    "\n",
    "\n",
    "index_name = \"docu-layout-index-01\" #Change to index name that you want to store to\n",
    "\n",
    "\n",
    "fields = [ #change fields to fields that you want to use\n",
    "        SimpleField(name= \"id\", type= 'Edm.String', key= True, filterable = True),\n",
    "        SimpleField(name = \"pageNumber\", type = \"Edm.Int32\", filterable = False, facetable = False, searchable = False, sortable = False),\n",
    "        SimpleField(name= \"parent\", type= 'Edm.String', filterable = True),\n",
    "        SearchableField(name= \"content\", type= 'Edm.String', searchable= True, filterable= True, facetable= True, sortable= True),\n",
    "        SearchableField(name= \"metadata\", type= 'Edm.String', searchable= True, filterable= True, facetable= False, sortable= True),\n",
    "        SimpleField(name = 'url', type = 'Edm.String', filterable = False, facetable = False, searchable = False, sortable = False), \n",
    "        SearchField(name= \"content_vector\", type= SearchFieldDataType.Collection(\"Edm.Single\"), searchable= True, vector_search_dimensions= 1536, vector_search_profile_name= \"myHnswProfile\"  ),\n",
    "    ]\n",
    "\n",
    "\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint= search_endpoint,\n",
    "    azure_search_key= search_admin_key,\n",
    "    index_name=index_name,\n",
    "    embedding_function=aoai_embeddings.embed_query,\n",
    "    fields= fields, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "def doc_intelligence(document) -> AnalyzeResult:\n",
    "    pdf_reader = PdfReader(io.BytesIO(document))\n",
    "    num_pages = pdf_reader._get_num_pages()\n",
    "    all_results = []\n",
    "    print(num_pages)\n",
    "\n",
    "    for start_page in range(1, num_pages, 2):\n",
    "        end_page = min(start_page + 2, num_pages)\n",
    "    \n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", \n",
    "            analyze_request=document, \n",
    "            content_type=\"application/pdf\", \n",
    "            output_content_format=\"markdown\",\n",
    "            pages = f\"{start_page} - {end_page}\" #if end_page != num_pages else f\"{start_page}\"\n",
    "        )\n",
    "        result: AnalyzeResult = poller.result()\n",
    "        all_results.append(result)\n",
    "    \n",
    "    return all_results\n",
    "    \n",
    "# Format intial dict\n",
    "def format_dict(name: str, url: str,):\n",
    "    \n",
    "    new_name = name.split(\".\")[0]\n",
    "    new_name = new_name.replace(\" \", \"_\")\n",
    "    result_dict ={\n",
    "        \n",
    "        \"id\": new_name,\n",
    "        \"parent\": name,\n",
    "        \"url\": url,\n",
    "    }\n",
    "    return result_dict\n",
    "\n",
    "# pages and document dict\n",
    "def pages_dict(first_dict: dict, result: AnalyzeResult, index: int):\n",
    "    \n",
    "    page_index = index + 1\n",
    "    page_index = page_index*2-1 # we are always passing two pages so to get first is \n",
    "    doc_pages = []         # index * 2 - 1. Then increment by one for second page. \n",
    "    \n",
    "    for page in result.pages:\n",
    "        \n",
    "        lines=[]\n",
    "        for line in page.lines:\n",
    "            lines.append(line.content)\n",
    "\n",
    "        page_dict = {\n",
    "            'id': f'{first_dict[\"id\"]}_{page_index}',\n",
    "            'parent': first_dict[\"parent\"],\n",
    "            'pageNumber': page.page_number,\n",
    "            'url': first_dict[\"url\"],\n",
    "            'content': ' '.join(lines)\n",
    "        }\n",
    "        doc_pages.append(page_dict)\n",
    "        \n",
    "        page_index += 1\n",
    "\n",
    "        \n",
    "    return doc_pages\n",
    "\n",
    "# create documents to send for AI search and tables to merge. \n",
    "def prep_to_send(doc_pages: list):\n",
    "    \n",
    "    docs_to_send = []\n",
    "    \n",
    "    for page in doc_pages:\n",
    "        metadata = {\n",
    "            \"id\": page['id'],\n",
    "            \"parent\": page[\"parent\"],\n",
    "            \"pageNumber\": page['pageNumber'],\n",
    "            \"url\": page['url'],\n",
    "        }\n",
    "        doc = Document(page_content=page[\"content\"])\n",
    "        doc.metadata = metadata\n",
    "        docs_to_send.append(doc)\n",
    "\n",
    "    return docs_to_send"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process From Blob Storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[{'id': 'Coen424_Fall_2024-Assignment_One_1', 'parent': 'Coen424 Fall 2024-Assignment One.pdf', 'pageNumber': 1, 'url': 'https://coen424storage.blob.core.windows.net/docu-layout-01/Coen424%20Fall%202024-Assignment%20One.pdf', 'content': 'COEN 424/6313 Assignment1 Fall 2024 Individual or Group of 2 or 3 Assignment due by October 26 23:59 @copyright Yan Liu 2024-2025 This assignment is originally developed by Yan Liu @ Concordia University. This assignment is only for the course teaching and education purpose. Any distribution of this document to the Internet that involves any profit-making purpose is not given the consent from the author. This assignment is designed to practise data model design, queries and communication through binary serialization and deserialization (gRPC). The dataset is in the JSON form for Novel Prizes since year 1901. api.nobelprize.org/v1/prize.json To illustrate an data sample in Redis, one prize data sample from the prize.json is added to the Redis database with a key called prizes: 1, type as JSON. After creating an index, the \\'category:economics\\' can be searched using the FT.SEARCH (index search). JSON prizes:1 Key Size: 1 KB Length: 3 TTL: No limit Last refresh: < 1 min C . { \"year\" : \"2023\" € \"category\" : \"chemistry\" Ề \"laureates\" : [ 10 \" : { ... } 10 \" : { ... } 00 \"2\" : { ... } 10 ] + >_ CLI - x > FT. SEARCH prizeIdx \\'@category : economics\\' 1) \"1\" 2) \"prizes : 2\" 3) 1) \"$\" 2) \"{\\\\\"year\\\\\":\\\\\"2023\\\\\",\\\\\"category\\\\\":\\\\\"economics\\\\\",\\\\\"laureates\\\\\":[{\\\\\"id\\\\\":\\\\\"1034\\\\\",\\\\\"firstname\\\\\":\\\\\"Claudia\\\\\",\\\\\"surname\\\\\":\\\\\"Goldin\\\\\",\\\\\"motivation\\\\\":\\\\\"\\\\\\\\\\\\\"for h aving advanced our understanding of women\\\\xe2\\\\x80\\\\x99s labour market outcomes\\\\\\\\\\\\\"\\\\\", \\\\\"share\\\\\":\\\\\"1\\\\\"} ]}\" Task 1 Data Storage and Query in Redis. 1.1 Load this dataset into Redis Cloud using the free tier including at least data in year 2013 to year 2023 inclusive. The keys\\' type should be JSON. 1.2 Create an index that includes the key \\'year\\' and \\'category\\' at least. [bonus point: the index includes the key \\'laureates\\' as vector type]. A reference on Redis index is accessible here. Indexing | Docs (redis.io) 1.3 Program a client application code with a language preferred to respond the following queries: · Given a category value, return the total number of laureates between a certain year range within the span from year 2013 to year 2023. The query should use the index. · Given a keyword, return the total number of laureates that have motivations covering the keyword.'}, {'id': 'Coen424_Fall_2024-Assignment_One_2', 'parent': 'Coen424 Fall 2024-Assignment One.pdf', 'pageNumber': 2, 'url': 'https://coen424storage.blob.core.windows.net/docu-layout-01/Coen424%20Fall%202024-Assignment%20One.pdf', 'content': \". Given the first name and last name, return the year, category and motivation of the laureate. This client program or application can run on the local computer that connects to Redis cloud for queries. The application can be run as an application from command line or IDE without the need of any UI. Task 2 - Data Model Communication through gRPC or any Protobuf based binary serialization and deserialization). The communication involves two parts - client and service gateway. A client sends a gRPC Request with parameters to the service gateway. The service gateway is a gRPC service. It invokes the data access components (similar to Task 1' programs) that retrieve data from the Redis cloud. The service gateway generates the gRPC Response and sends the response back to the client. The reference to gRPC is accessible from gRPC. A reference below on programing null object as Empty request or response. Google.Protobuf.WellKnownTypes.Empty Class Reference [ https://protobuf.dev/reference/csharp/api- docs/class/google/protobuf/well-known-types/empty ] Data Access Query Component Request Redis Cloud gRPC O gRPC Client program Data Access Query Component Response Gateway (gRPC Service) 2.1 Define the three queries in Task 1.3 into three gRPC services with. proto files. 2.2 Define the JSON data model for Request and Response for each query. Use a json to proto converter to generate the Response and Request Protobuf messages. 2.3 Choose the programming language preferred, program the gRPC client and gRPC service or gateway. This gateway includes data access query components refactored or reused from Task 1 to query the results from the Redis cloud. 2.4 Deploy the gRPC service to a cloud endpoint . The gRPC client can run on local computer. 2.5 Run the client from a local computer for 100 times to each of three queries and measure the end-to- end delay. 2.6 Plot the box plots for each query's 100 measured end-to-end delays. An example is shown below.\"}]\n",
      "To Send:  Coen424_Fall_2024-Assignment_One_1\n",
      "To Send:  Coen424_Fall_2024-Assignment_One_2\n",
      "Document Coen424 Fall 2024-Assignment One.pdf uploaded.\n",
      "[{'id': 'Coen424_Fall_2024-Assignment_One_3', 'parent': 'Coen424 Fall 2024-Assignment One.pdf', 'pageNumber': 3, 'url': 'https://coen424storage.blob.core.windows.net/docu-layout-01/Coen424%20Fall%202024-Assignment%20One.pdf', 'content': \"wbs W@S scale Upper whisker 325 25% Quartile group 4 300 Upper quartile 25% Quartile group 3 275 Middle quartile / median 25% Quartile group 2 250 Lower quartile 25% Quartile group 1 225 Lower whisker 200 Task 3: Write up the solution report following the Template IEEE - Manuscript Templates for Conference Proceedings https://www.ieee.org/conferences/publishing/templates.html The report content outline Section 1. Redis Cloud Query Solution Describing the technical solutions for Task 1.1 and 1.2. Attach screenshots for 1.3's queries and outputs for each query. Section 2. Data Model Design Service Development 2.1 The overview of serialization and deserialization model (gRPC or protobuf based framework) 2.2 Present the .proto for each gRPC service and messages defined. 2.3 For each service definition, screenshot of the code implementation with brief description Section 3. Cloud Deployment and Run Present a step by step description on the deployment to a cloud at your choice and present how the endpoint of the cloud is set or configured. Attach a screenshot of the running cloud service, and returning responses to the client's requests. Section 4. 4.1 The settings to run 100 times of each service\"}, {'id': 'Coen424_Fall_2024-Assignment_One_4', 'parent': 'Coen424 Fall 2024-Assignment One.pdf', 'pageNumber': 4, 'url': 'https://coen424storage.blob.core.windows.net/docu-layout-01/Coen424%20Fall%202024-Assignment%20One.pdf', 'content': \"4.2 The box plots with measurement of three gRPC services. Section 5 Member contribution list of each member's contribution according to the checklist below. Name (SID) and Signature Task List Contribution Role and Percentage ( X % ) (If Y for the task list, write the contribution role and percentage counted for the completeness of this task) Member 1 Task 1.1 Y / N Task 1.2 Y / N Task 1.3 Y / N Task 2.1 Y / N Task 2.2 Y / N Task 2.3 Y / N Task 2.4 Y / N Task 2.5 Y / N Task 2.6 Y/ N Task 3 Y / N Create [SID_1]_[SID_2]_[SID_3]A1.zip (.gz, .tar, .zip are acceptable. . rar file is NOT acceptable) file contains a folder named with your student ID. This folder contains all the source code + report.pdf for the assignment. One submission for the group is sufficient.\"}]\n",
      "To Send:  Coen424_Fall_2024-Assignment_One_3\n",
      "To Send:  Coen424_Fall_2024-Assignment_One_4\n",
      "Document Coen424 Fall 2024-Assignment One.pdf uploaded.\n"
     ]
    }
   ],
   "source": [
    "processed_names = [\"Coen424 Fall 2024-Assignment One\"]\n",
    "list_blobs = blob_client.list_blobs()\n",
    "for blob in list_blobs:\n",
    "   \n",
    "    # get blob data and info \n",
    "    \n",
    "    blob_name = blob.name\n",
    "    if blob_name in processed_names: \n",
    "        continue\n",
    "    processed_names.append(blob_name)\n",
    "    blob_url = (blob_client.get_blob_client(blob_name)).url\n",
    "    blob_data = bytes(blob_client.download_blob(blob_name).readall())\n",
    "    \n",
    "    try:\n",
    "    # pass blob data through document intelligence\n",
    "        doc_data = doc_intelligence(document = blob_data)    \n",
    "        # first dict format\n",
    "        first_dict = format_dict(name = blob_name, url = blob_url)\n",
    "        #doc_data = doc_data\n",
    "        for d, data in enumerate(doc_data):\n",
    "        # pages dict \n",
    "            doc_pages = pages_dict(first_dict = first_dict, result = data, index = d)\n",
    "            print(doc_pages)\n",
    "        # docs to send and tables to merge \n",
    "            docs_to_send = prep_to_send(doc_pages = doc_pages)\n",
    "        # vector store document\n",
    "            #documents_to_vector.append(docs_to_send)\n",
    "\n",
    "            vector_store.add_documents(docs_to_send)\n",
    "        # merge tables\n",
    "        # batch = IndexDocumentsBatch()\n",
    "        # if tables_to_merge:\n",
    "        #     batch.add_merge_actions(*tables_to_merge)\n",
    "        #     search_client.index_documents(batch)\n",
    "    \n",
    "            print(f\"Document {blob_name} uploaded.\")\n",
    "            #print(doc_pages)\n",
    "    \n",
    "    except Exception as e: \n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"Error occured, file name: {blob_name}\")\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
