{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tabulate import tabulate\n",
    "import io\n",
    "import copy\n",
    "import re\n",
    "# Azure imports\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient \n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from azure.search.documents import SearchClient, IndexDocumentsBatch\n",
    "from azure.search.documents.indexes.models import(\n",
    "    SimpleField,\n",
    "    ComplexField,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .env Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_env():\n",
    "    from dotenv import load_dotenv, dotenv_values\n",
    "    import os\n",
    "\n",
    "    # Clear the current environment variables\n",
    "    for key in dotenv_values().keys():\n",
    "        os.environ.pop(key, None)\n",
    "\n",
    "    # Reload the .env file\n",
    "    load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code loads environment variables using the `dotenv` library and sets the necessary environment variables for Azure services.\n",
    "The environment variables are loaded from the `.env` file in the same directory as this notebook.\n",
    "\"\"\"\n",
    "\n",
    "load_dotenv()\n",
    "#reload_env()\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"OPENAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"OPENAI_KEY\")\n",
    "doc_intelligence_endpoint = os.getenv(\"DOC_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"DOC_INTELLIGENCE_KEY\")\n",
    "blob_connection_string = os.getenv(\"BLOB_STORAGE_CONNECTION_STRING\")\n",
    "storage_account_key = os.getenv(\"STORAGE_ACCOUNT_KEY\")\n",
    "search_endpoint: str = os.getenv(\"AI_SEARCH_ENDPOINT\")\n",
    "search_admin_key: str = os.getenv(\"AI_SEARCH_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index name\n",
    "index_name: str = \"docu-layout-01\" #Change to index name that you want to store to\n",
    "\n",
    "# Connect to document intelligence\n",
    "document_intelligence_client = DocumentIntelligenceClient(endpoint= doc_intelligence_endpoint, credential=AzureKeyCredential(doc_intelligence_key))\n",
    "\n",
    "# Connect to blob storage \n",
    "blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
    "\n",
    "blob_client = blob_service_client.get_container_client(\"docu-layout-01\")\n",
    "if not blob_client.exists():\n",
    "    blob_client.create_container()\n",
    "    print(\"Created\")\n",
    "list_blobs = blob_client\n",
    "\n",
    "# Connect to search service \n",
    "search_client = SearchClient(endpoint = search_endpoint, index_name = index_name, credential = AzureKeyCredential(search_admin_key))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to Blob Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: Coen424 Fall 2024-Assignment One.pdf as <azure.storage.blob._blob_client.BlobClient object at 0x00000200FECC3E30>\n"
     ]
    }
   ],
   "source": [
    "files = [\"D:\\\\OneDrive\\\\Semester 7 COEN\\\\COEN 424\\\\Project\\\\Code\\\\COEN-424-DocuLayout\\\\Coen424 Fall 2024-Assignment One.pdf\"]\n",
    "\n",
    "# Send files\n",
    "for index, file_to_send in enumerate(files):\n",
    "    with open(file_to_send, mode = \"rb\") as data:\n",
    "        sent = blob_client.upload_blob(name=os.path.basename(file_to_send), data=data, overwrite=True) #should also add metadata\n",
    "    print(f\"Sent: {os.path.basename(file_to_send)} as {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect openAI and search service \n",
    "aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2023-12-01-preview\",  # e.g., \"2023-12-01-preview\"\n",
    ")\n",
    "\n",
    "\n",
    "index_name = \"docu-layout-index-01\" #Change to index name that you want to store to\n",
    "\n",
    "\n",
    "fields = [ #change fields to fields that you want to use\n",
    "        SimpleField(name= \"id\", type= 'Edm.String', key= True, filterable = True),\n",
    "        SimpleField(name = \"pageNumber\", type = \"Edm.Int32\", filterable = False, facetable = False, searchable = False, sortable = False),\n",
    "        SimpleField(name= \"parent\", type= 'Edm.String', filterable = True),\n",
    "        SearchableField(name= \"content\", type= 'Edm.String', searchable= True, filterable= True, facetable= True, sortable= True),\n",
    "        SearchableField(name= \"metadata\", type= 'Edm.String', searchable= True, filterable= True, facetable= False, sortable= True),\n",
    "        SimpleField(name = 'url', type = 'Edm.String', filterable = False, facetable = False, searchable = False, sortable = False), \n",
    "        SearchField(name= \"content_vector\", type= SearchFieldDataType.Collection(\"Edm.Single\"), searchable= True, vector_search_dimensions= 1536, vector_search_profile_name= \"myHnswProfile\"),\n",
    "    ]\n",
    "\n",
    "\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint= search_endpoint,\n",
    "    azure_search_key= search_admin_key,\n",
    "    index_name=index_name,\n",
    "    embedding_function=aoai_embeddings.embed_query,\n",
    "    fields= fields\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import time\n",
    "def doc_intelligence(document) -> AnalyzeResult:\n",
    "    pdf_reader = PdfReader(io.BytesIO(document))\n",
    "    num_pages = pdf_reader._get_num_pages()\n",
    "    all_results = []\n",
    "    print(num_pages)\n",
    "\n",
    "    for start_page in range(1, num_pages+1, 2):\n",
    "        end_page = min(start_page + 2, num_pages)\n",
    "    \n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", \n",
    "            analyze_request=document, \n",
    "            content_type=\"application/pdf\", \n",
    "            output_content_format=\"markdown\",\n",
    "            pages = f\"{start_page} - {end_page}\" #if end_page != num_pages else f\"{start_page}\"\n",
    "        )\n",
    "        result: AnalyzeResult = poller.result()\n",
    "        all_results.append(result)\n",
    "    \n",
    "    return all_results\n",
    "    \n",
    "# Format intial dict\n",
    "def format_dict(name: str, url: str,):\n",
    "    \n",
    "    new_name = name.split(\".\")[0]\n",
    "    new_name = new_name.replace(\" \", \"_\")\n",
    "    result_dict ={\n",
    "        \n",
    "        \"id\": new_name,\n",
    "        \"parent\": name,\n",
    "        \"url\": url,\n",
    "    }\n",
    "    return result_dict\n",
    "\n",
    "# pages and document dict\n",
    "def pages_dict(first_dict: dict, result: AnalyzeResult, index: int):\n",
    "    \n",
    "    page_index = index + 1\n",
    "    page_index = page_index*2-1 # we are always passing two pages so to get first is \n",
    "    doc_pages = []         # index * 2 - 1. Then increment by one for second page. \n",
    "    \n",
    "    for page in result.pages:\n",
    "        \n",
    "        lines=[]\n",
    "        for line in page.lines:\n",
    "            lines.append(line.content)\n",
    "\n",
    "        page_dict = {\n",
    "            'id': f'{first_dict[\"id\"]}_{page_index}',\n",
    "            'parent': first_dict[\"parent\"],\n",
    "            'pageNumber': page.page_number,\n",
    "            'url': first_dict[\"url\"],\n",
    "            'content': ' '.join(lines)\n",
    "        }\n",
    "        doc_pages.append(page_dict)\n",
    "        \n",
    "        page_index += 1\n",
    "\n",
    "        \n",
    "    return doc_pages\n",
    "\n",
    "# create documents to send for AI search and tables to merge. \n",
    "def prep_to_send(doc_pages: list):\n",
    "    \n",
    "    docs_to_send = []\n",
    "    \n",
    "    for page in doc_pages:\n",
    "        metadata = {\n",
    "            \"id\": page['id'],\n",
    "            \"parent\": page[\"parent\"],\n",
    "            \"pageNumber\": page['pageNumber'],\n",
    "            \"url\": page['url'],\n",
    "        }\n",
    "        doc = Document(page_content=page[\"content\"])\n",
    "        doc.metadata = metadata\n",
    "        docs_to_send.append(doc)\n",
    "\n",
    "    return docs_to_send"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process From Blob Storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[{'id': 'Project_Description-Fall2024-version2_1', 'parent': 'Project Description-Fall2024-version2.0.pdf', 'pageNumber': 1, 'url': 'https://doculayoutstorage.blob.core.windows.net/docu-layout-01/Project%20Description-Fall2024-version2.0.pdf', 'content': 'Concordia University Department of Electrical and Computer Engineering COEN424/COEN6313 Programming on Cloud Fall 2024 Project Description Overview In this course, you will be working in groups of 4 or less members on a final project that runs on the Cloud. Your project should develop a service that has targeted users and with specific functions. Example scenarios are given below. If you own proposed scenarios are not approved, you need to choose one of the following scenarios. A service is accessible on the Internet. Its design should follow RESTful architecture style that runs on a Cloud platform and meets the requirements below. The service interface is defined using SwaggerHub open APIs. Throughout the semester, you will work on your project and to present and discuss it with the class. During the final week of the semester you will present and demonstrate your project to the class and possibly some outside evaluators. Theme 1: Integration with Cloud AI Services or Huggingface Models Individuals, organizations, and companies have the needs of adopting AI services. Cloud and open community provide pre-trained models that can be leveraged to deliver smart services. A service can integrate with these open cloud services or a huggingface model to deliver the cloud enable development, configuration and integration of multiple types of services. Theme 2: Big Data Analysis There are many open datasets available. The analytics can benefit from distributed parallel programming models such as MapReduce and Spark. A big data analysis project that is leveraging MapReduce, Machine Learning frameworks and deployed on Cloud is also an option, it still needs to address the requirements below. Mandatory Requirements Your project must include the following: 1. Utilize a Cloud platform such as Amazon EC2, Google App Engine, Windows Azure, Joyent, Linode, Heroku, Rackspace, Eucalyptus etc or setup a private'}, {'id': 'Project_Description-Fall2024-version2_2', 'parent': 'Project Description-Fall2024-version2.0.pdf', 'pageNumber': 2, 'url': 'https://doculayoutstorage.blob.core.windows.net/docu-layout-01/Project%20Description-Fall2024-version2.0.pdf', 'content': 'cloud by yourself on your own computers. Take advantage of a platform- specific feature. For instance, if you use Amazon EC2, you can take advantage of S3 or DynamoDB. 2. Design the data model of your services. 3. Use a NoSQL data store for managing the data of your application. 4. Select a Hugging face model or a Cloud AI service to integrate 5. Develop and deploy the service with integration to 4. 6. Github of all the source code and report shared to the PoD and lecturer. PLEASE BE CAREFUL, if you decide to use a public repository to host your source code, please DO NOT save your public cloud account keys to the repository, nor to put the plain text of your account keys and tokens into the source code. The leaking of your account information is the same as leaking your credit card information publicly. A possible fraud can incur with unexpected high cost in your billing of using the public cloud. When you use a public cloud, you should be aware of the possible cost and you are responsible for the cost incurred by yourself. Extra Feature Your project must select ONE of the following features to implement: 1. Expose REST/Open APIs to SwaggerHub 2. Use Docker and/or DevOps for the project CI/CD 3. Do performance and operation measurement Schedule This project includes the following deliverables and time schedule. D1 Week 2 Tuesday 23:55 : Group information submitted Find project partners and discuss your possible project ideas and what sort of platform you wish to use. The size of a group is 3 or 4. Please select your group leader. The group leader should submit to Moodle site a single pdf or txt file with the file name D1-[SID of Member]- [SID of Member]- [SID of Member].pdf or D1-[SID of Member]- [SID of Member]- [SID of Member].txt'}]\n",
      "[{'id': 'Project_Description-Fall2024-version2_3', 'parent': 'Project Description-Fall2024-version2.0.pdf', 'pageNumber': 3, 'url': 'https://doculayoutstorage.blob.core.windows.net/docu-layout-01/Project%20Description-Fall2024-version2.0.pdf', 'content': 'containing the following information · For each group please indicate the following information of each member [SID] [First and Last Name] [Undergraduate or Graduate] [Project Leader Yes/No] D2 Week 3 Tuesday: Group Presentation [5 points] D2.1 At the class time, each group will give a 3-minute presentation on their project. Be sure to address the following questions: 1. Title of your project 2. What is the problem your project is trying to solve? 3. Who would be the end users of your web service? 3. What kind of applications can be further developed by using your services? 4. How would you meet the project requirements? Please present your technical choice and development methods and project management tools. 5. What would you want to do and what will you actually accomplish? This group presentation is compulsory. Without the presentation, the project scope will not be approved by the lecturer. As a result, this may lead to a complete failure of the whole project. Due to the limitation of class time, we will only select a number of groups for presenting in turn at the class time. Other groups will make a recording within 5 minutes and share the video links at the first title page in your submission slides due by Tuesday 23:55. One representative member of each group gives the talk. Others may be asked to answer questions. D2.2 Week 3 Tuesday 23:55, the presentation slides from each group should be uploaded to Moodle site, given the file name as D2-[GroupID].pdf or D2-[GroupID].ppt D3 Week 8 Tuesday: Progress Report [5 points] D3.1 At the class time, each group should give a 3-minute presentation to cover the following'}, {'id': 'Project_Description-Fall2024-version2_4', 'parent': 'Project Description-Fall2024-version2.0.pdf', 'pageNumber': 4, 'url': 'https://doculayoutstorage.blob.core.windows.net/docu-layout-01/Project%20Description-Fall2024-version2.0.pdf', 'content': '· Title of your project · Main function of your SaaS · Architecture design (e.g. UML diagrams to represent the structure of the design, interaction and deployment) · Technologies used and what have you accomplished so far? · What issues have you faced and how have you overcome them? · What do you have left to do? · Are your goals still the same? D3.2 Week 8 Tuesday 23:55, the presentation slides from each group should be uploaded to Moodle site. The presentation slide should be submitted the following name convention D3-[GroupID].pdf or D3-[GroupID].ppt D4 Week 12 Tuesday: Final Presentation, Demo and Report (20 points) D4.1 At the class time, each group will give a 15-minute presentation and demonstration of their project. Be sure to address the following questions: 1. What is your web service? 2. Your architecture design and use of cloud technologies (be sure to address the project\\'s mandatory requirement) · Does your architecture follow the REST architecture style? · Please present our data model, storage and access to data. . Be sure to describe the architecture of your application and the various components. 3. What extra feature(s) have you achieved? 4. Techniques you applied for implementation 5. What works and what doesn\\'t work? 6. Discussion on how your architecture can address one or more quality attributes of performance, scalability, availability, and security 7. Discussion on the strengths of the cloud technologies you applied and what limitation you have experienced so far. 8. Live demo (in case of failure, prepare a backup video demo) Each member should perform one of the activities of presentation talk, live demo and Q&A. D4.2 Week 12 Tuesday 23:55, each group will submit a) A single report file to contain the following \" Title of the project · Abstract · Introduction (including goal, objectives, problem statement, assumptions, methodology, and schedule)'}]\n",
      "[{'id': 'Project_Description-Fall2024-version2_5', 'parent': 'Project Description-Fall2024-version2.0.pdf', 'pageNumber': 5, 'url': 'https://doculayoutstorage.blob.core.windows.net/docu-layout-01/Project%20Description-Fall2024-version2.0.pdf', 'content': '· Make sure to highlight each of these aspects in the introduction (e.g., use subsections). · Project Description: · Function provided by the service · Architectural design · Technical implementation details . The URL to access your project source code & URL to access your web service · Discussion: · Discussion on the experience, observations, and lessons learnt Include Figures (if applicable): e.g., scalability, security level, quality of service, etc. · Contribution of each member: . The role and technical contribution of each member · Reference to technical and academicarticles · The URL to access your project source code · The URL to access your web service The report should be within 5 pages including all the figures and references, following the format of http://www.ieee.org/conferences_events/conferences/publishing/templates.html The file name should follow the following naming convention D4-report-[GroupID].pdf b) The presentation slides in ppt or pdf with the following naming convention D4-presentation-[GroupID].pdf or D4-presentation-[GroupID].ppt c) All the source code of your project in a single package. D4-project-[GroupID].tar or D4-project-[GroupID].zip'}]\n",
      "Document Project Description-Fall2024-version2.0.pdf uploaded.\n"
     ]
    }
   ],
   "source": [
    "processed_names = [\"\"]\n",
    "list_blobs = blob_client.list_blobs()\n",
    "for blob in list_blobs:\n",
    "   \n",
    "    # get blob data and info \n",
    "    \n",
    "    blob_name = blob.name\n",
    "    if blob_name in processed_names: \n",
    "        continue\n",
    "    processed_names.append(blob_name)\n",
    "    blob_url = (blob_client.get_blob_client(blob_name)).url\n",
    "    blob_data = bytes(blob_client.download_blob(blob_name).readall())\n",
    "    \n",
    "    try:\n",
    "    # pass blob data through document intelligence\n",
    "        doc_data = doc_intelligence(document = blob_data)    \n",
    "        # first dict format\n",
    "        first_dict = format_dict(name = blob_name, url = blob_url)\n",
    "        #doc_data = doc_data\n",
    "        for d, data in enumerate(doc_data):\n",
    "        # pages dict \n",
    "            doc_pages = pages_dict(first_dict = first_dict, result = data, index = d)\n",
    "            print(doc_pages)\n",
    "        # docs to send and tables to merge \n",
    "            docs_to_send = prep_to_send(doc_pages = doc_pages)\n",
    "        # vector store document\n",
    "            #documents_to_vector.append(docs_to_send)\n",
    "\n",
    "            vector_store.add_documents(docs_to_send)\n",
    "        # merge tables\n",
    "        # batch = IndexDocumentsBatch()\n",
    "        # if tables_to_merge:\n",
    "        #     batch.add_merge_actions(*tables_to_merge)\n",
    "        #     search_client.index_documents(batch)\n",
    "    \n",
    "        print(f\"Document {blob_name} uploaded.\")\n",
    "            #print(doc_pages)\n",
    "        time.sleep(1)\n",
    "    except Exception as e: \n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"Error occured, file name: {blob_name}\")\n",
    "        continue\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
