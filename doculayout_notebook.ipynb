{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from tabulate import tabulate\n",
    "import io\n",
    "import copy\n",
    "import re\n",
    "# Azure imports\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient \n",
    "from azure.ai.documentintelligence.models import AnalyzeResult\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from azure.search.documents import SearchClient, IndexDocumentsBatch\n",
    "from azure.search.documents.indexes.models import(\n",
    "    SimpleField,\n",
    "    ComplexField,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code loads environment variables using the `dotenv` library and sets the necessary environment variables for Azure services.\n",
    "The environment variables are loaded from the `.env` file in the same directory as this notebook.\n",
    "\"\"\"\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "doc_intelligence_endpoint = \"https://cog-fr-m42nz7gwiimnc.cognitiveservices.azure.com/\"\n",
    "doc_intelligence_key = \"f763f17a17fc4acb84cc47afe21423f1\"\n",
    "blob_connection_string = os.getenv(\"AZURE_BLOB_STORAGE_CONNECTION_STRING\")\n",
    "storage_account_key = os.getenv(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "search_endpoint: str = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "search_admin_key: str = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index name\n",
    "index_name: str = \"index-publications-02\" #Change to index name that you want to store to\n",
    "\n",
    "# Connect to document intelligence\n",
    "document_intelligence_client = DocumentIntelligenceClient(endpoint= doc_intelligence_endpoint, credential=AzureKeyCredential(doc_intelligence_key))\n",
    "\n",
    "# Connect to blob storage \n",
    "blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
    "blob_client = blob_service_client.get_container_client(\"publications\")\n",
    "list_blobs = blob_client\n",
    "\n",
    "# Connect to search service \n",
    "search_client = SearchClient(endpoint = search_endpoint, index_name = index_name, credential = AzureKeyCredential(search_admin_key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect openAI and search service \n",
    "aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=\"text-embedding-ada-002-32k\",\n",
    "    openai_api_version=\"2023-12-01-preview\",  # e.g., \"2023-12-01-preview\"\n",
    ")\n",
    "\n",
    "\n",
    "index_name = \"index-publications-02\" #Change to index name that you want to store to\n",
    "\n",
    "\n",
    "fields = [ #change fields to fields that you want to use\n",
    "        SimpleField(name= \"id\", type= 'Edm.String', key= True, filterable = True),\n",
    "        SimpleField(name = \"pageNumber\", type = \"Edm.Int32\", filterable = False, facetable = False, searchable = False, sortable = False),\n",
    "        SimpleField(name= \"parent\", type= 'Edm.String', filterable = True),\n",
    "        SearchableField(name= \"content\", type= 'Edm.String', searchable= True, filterable= True, facetable= True, sortable= True),\n",
    "        SearchableField(name = \"tables\", type = \"Edm.String\", collection= True, searchable= True, filterable= False, facetable= False, sortable= False),\n",
    "        SearchableField(name= \"metadata\", type= 'Edm.String', searchable= True, filterable= True, facetable= False, sortable= True),\n",
    "        SimpleField(name = 'url', type = 'Edm.String', filterable = False, facetable = False, searchable = False, sortable = False), \n",
    "        SearchField(name= \"content_vector\", type= SearchFieldDataType.Collection(\"Edm.Single\"), searchable= True, vector_search_dimensions= 1536, vector_search_profile_name= \"myHnswProfile\"  ),\n",
    "    ]\n",
    "\n",
    "\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint= search_endpoint,\n",
    "    azure_search_key= search_admin_key,\n",
    "    index_name=index_name,\n",
    "    embedding_function=aoai_embeddings.embed_query,\n",
    "    fields= fields, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_names = []\n",
    "list_blobs = blob_client.list_blobs()\n",
    "for blob in list_blobs:\n",
    "   \n",
    "    # get blob data and info \n",
    "    \n",
    "    blob_name = blob.name\n",
    "    if blob_name in processed_names: \n",
    "        continue\n",
    "    processed_names.append(blob_name)\n",
    "    blob_url = (blob_client.get_blob_client(blob_name)).url\n",
    "    blob_data = bytes(blob_client.download_blob(blob_name).readall())\n",
    "    \n",
    "    try:\n",
    "    # pass blob data through document intelligence\n",
    "        doc_data = doc_intelligence(document = blob_data)\n",
    "    # extract tables from document\n",
    "        tables = get_table(result = doc_data)\n",
    "    # first dict format\n",
    "        first_dict = format_dict(name = blob_name, url = blob_url)\n",
    "    # pages and tables dict \n",
    "        doc_pages = pages_tables_dict(first_dict = first_dict, result = doc_data, tables = tables)\n",
    "    # docs to send and tables to merge \n",
    "        docs_to_send, tables_to_merge = prep_to_send(doc_pages = doc_pages)\n",
    "    # vector store document\n",
    "        vector_store.add_documents(docs_to_send)\n",
    "    # merge tables\n",
    "        batch = IndexDocumentsBatch()\n",
    "        if tables_to_merge:\n",
    "            batch.add_merge_actions(*tables_to_merge)\n",
    "            search_client.index_documents(batch)\n",
    "    \n",
    "        print(f\"Document {blob_name} uploaded.\")\n",
    "        print(doc_pages)\n",
    "    \n",
    "    except: \n",
    "        print(f\"Error occured, file name: {blob_name}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document intelligence functions\n",
    "def doc_intelligence(document) -> AnalyzeResult:\n",
    "    \n",
    "    poller = document_intelligence_client.begin_analyze_document(\n",
    "    \"prebuilt-layout\", \n",
    "    analyze_request = document, \n",
    "    content_type = \"application/pdf\", \n",
    "    output_content_format = \"markdown\")\n",
    "    result: AnalyzeResult = poller.result()\n",
    "    return result\n",
    "\n",
    "# Get tables from document processed with document intelligence \n",
    "def get_table(result: AnalyzeResult):\n",
    "    tables = []\n",
    "    table_formated = []\n",
    "    if result.tables:\n",
    "        for table in result.tables:\n",
    "            table_data = []\n",
    "            headers = []\n",
    "            \n",
    "            for cell in table.cells:\n",
    "                if cell.row_index == 0:\n",
    "                    headers.append(cell.content)\n",
    "                else:\n",
    "                    if len(table_data) < cell.row_index:\n",
    "                        table_data.append([])\n",
    "                    table_data[cell.row_index-1].append(cell.content)\n",
    "            \n",
    "            table_formated.append(tabulate(table_data, headers=headers, tablefmt=\"simple\")) ## store tables for storage\n",
    "        tables.append(table_formated)\n",
    "    return tables\n",
    "\n",
    "# Format intial dict\n",
    "def format_dict(name: str, url: str):\n",
    "    \n",
    "    new_name = name.split(\".\")[0]\n",
    "    result_dict ={\n",
    "        \n",
    "        \"id\": new_name,\n",
    "        \"parent\": name,\n",
    "        \"url\": url,\n",
    "    }\n",
    "    return result_dict\n",
    "\n",
    "# pages and document dict\n",
    "def pages_tables_dict(first_dict: dict, result: AnalyzeResult, tables: list):\n",
    "    \n",
    "    doc_pages = []\n",
    "    doc_tables = []\n",
    "\n",
    "    if result.tables:\n",
    "        for t, tabs in enumerate(result.tables):\n",
    "            for page in tabs.bounding_regions:\n",
    "                table_dict = {\n",
    "                    'id': f'{first_dict[\"id\"]}_{page.page_number}',\n",
    "                    'pageNumber': page.page_number,\n",
    "                    'table': tables[0][t]\n",
    "                }\n",
    "                doc_tables.append(table_dict)\n",
    "\n",
    "    for page in result.pages:\n",
    "        lines=[]\n",
    "        for line in page.lines:\n",
    "            lines.append(line.content)\n",
    "\n",
    "        page_dict = {\n",
    "            'id': f'{first_dict[\"id\"]}_{page.page_number}',\n",
    "            'parent': first_dict[\"parent\"],\n",
    "            'pageNumber': page.page_number,\n",
    "            'url': first_dict[\"url\"],\n",
    "            'tables': [],\n",
    "            'content': ' '.join(lines)\n",
    "        }\n",
    "\n",
    "        for tabs in doc_tables:\n",
    "            if tabs['pageNumber'] == page.page_number:\n",
    "                page_dict['tables'].append(tabs['table'])\n",
    "        doc_pages.append(page_dict)\n",
    "        \n",
    "    return doc_pages\n",
    "\n",
    "# create documents to send for AI search and tables to merge. \n",
    "def prep_to_send(doc_pages: list):\n",
    "    \n",
    "    docs_to_send = []\n",
    "    tables_to_merge = []\n",
    "    \n",
    "    for page in doc_pages:\n",
    "        metadata = {\n",
    "            \"id\": page['id'],\n",
    "            \"parent\": page[\"parent\"],\n",
    "            \"pageNumber\": page['pageNumber'],\n",
    "            \"url\": page['url'],\n",
    "        }\n",
    "        doc = Document(page_content=page[\"content\"])\n",
    "        doc.metadata = metadata\n",
    "        docs_to_send.append(doc)\n",
    "        \n",
    "        tabs = {\n",
    "            \"id\": page['id'],            \n",
    "            'tables': page['tables']\n",
    "        }\n",
    "        tables_to_merge.append(tabs)\n",
    "\n",
    "    return docs_to_send, tables_to_merge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
